loki:
  enabled: true
  fullnameOverride: loki
  
  loki:
    commonConfig:
      replication_factor: 1
    storage:
      type: 'filesystem'
      filesystem:
        chunks_directory: /var/loki/chunks
        rules_directory: /var/loki/rules
    schemaConfig:
      configs:
      - from: "2024-01-01"
        store: tsdb
        index:
          prefix: loki_index_
          period: 24h
        object_store: filesystem
        schema: v13
    ingester:
      chunk_encoding: snappy
    tracing:
      enabled: true
    querier:
      # Default is 4, if you have enough memory and CPU you can increase, reduce if OOMing
      max_concurrent: 2
  deploymentMode: SingleBinary
  singleBinary:
    replicas: 1
    resources:
      limits:
        cpu: 500m
        memory: 1Gi
      requests:
        cpu: 200m
        memory: 512Mi
    extraEnv:
      # Keep a little bit lower than memory limits
      - name: GOMEMLIMIT
        value: 1100MiB
  chunksCache:
    # default is 500MB, with limited memory keep this smaller
    writebackSizeLimit: 10MB
    allocatedMemory: 512
  read:
    replicas: 0
  backend:
    replicas: 0
  write:
    replicas: 0
  ingress:
    enabled: true
    ingressClassName: "traefik"
    annotations: {}
    labels: {}
    paths:
      distributor:
        - /ready
        - /api/prom/push
        - /loki/api/v1/push
        - /otlp/v1/logs
      queryFrontend:
        - /api/prom/query
        # this path covers labels and labelValues endpoints
        - /api/prom/label
        - /api/prom/series
        - /api/prom/tail
        - /loki/api/v1/query
        - /loki/api/v1/query_range
        - /loki/api/v1/tail
        # this path covers labels and labelValues endpoints
        - /loki/api/v1/label
        - /loki/api/v1/labels
        - /loki/api/v1/series
        - /loki/api/v1/index/stats
        - /loki/api/v1/index/volume
        - /loki/api/v1/index/volume_range
        - /loki/api/v1/format_query
        - /loki/api/v1/detected_field
        - /loki/api/v1/detected_fields
        - /loki/api/v1/detected_labels
        - /loki/api/v1/patterns
      ruler:
        - /api/prom/rules
        - /api/prom/api/v1/rules
        - /api/prom/api/v1/alerts
        - /loki/api/v1/rules
        - /prometheus/api/v1/rules
        - /prometheus/api/v1/alerts
    # -- Hosts configuration for the ingress, passed through the `tpl` function to allow templating
    hosts:
      - loki.k8s.test
  ingester:
    replicas: 0
  querier:
    replicas: 0
  queryFrontend:
    replicas: 0
  queryScheduler:
    replicas: 0
  distributor:
    replicas: 0
  compactor:
    replicas: 0
  indexGateway:
    replicas: 0
  bloomCompactor:
    replicas: 0
  bloomGateway:
    replicas: 0
tempo:
  enabled: true
  global:
    # -- Common labels for all object directly managed by this chart.
    commonLabels: {}

  # -- Overrides the chart's name
  nameOverride: ""

  # -- Overrides the chart's computed fullname
  fullnameOverride: "tempo"

  # -- Define the amount of instances
  replicas: 1

  # -- Number of old history to retain to allow rollback (If not set, default Kubernetes value is set to 10)
  # revisionHistoryLimit: 1

  # -- labels for tempo
  labels: {}

  # -- Annotations for the StatefulSet
  annotations: {}

  tempo:
    repository: grafana/tempo
    tag: ""
    pullPolicy: IfNotPresent
    ## Optionally specify an array of imagePullSecrets.
    ## Secrets must be manually created in the namespace.
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
    ##
    # pullSecrets:
    #   - myRegistryKeySecretName

    updateStrategy: RollingUpdate
    resources: {}
    #  requests:
    #    cpu: 1000m
    #    memory: 4Gi
    #  limits:
    #    cpu: 2000m
    #    memory: 6Gi

    memBallastSizeMbs: 1024
    multitenancyEnabled: false
    # -- If true, Tempo will report anonymous usage data about the shape of a deployment to Grafana Labs
    reportingEnabled: true
    metricsGenerator:
      # -- If true, enables Tempo's metrics generator (https://grafana.com/docs/tempo/next/metrics-generator/)
      enabled: false
      remoteWriteUrl: "http://prometheus:9090/api/v1/write"
    # -- Configuration options for the ingester.
    # Refers to: https://grafana.com/docs/tempo/latest/configuration/#ingester
    ingester: {}
    #  flush_check_period: 10s
    #  trace_idle_period: 10s
    #  max_block_duration: 30m
    #  complete_block_timeout: 1h
    # -- Configuration options for the querier.
    # Refers to: https://grafana.com/docs/tempo/latest/configuration/#querier
    querier: {}
    #  max_concurrent_queries: 20
    # -- Configuration options for the query-fronted.
    # Refers to: https://grafana.com/docs/tempo/latest/configuration/#query-frontend
    queryFrontend: {}
    #  search:
    #    concurrent_jobs: 2000
    retention: 24h
    # -- The standard overrides configuration section. This can include a `defaults` object for applying to all tenants (not to be confused with the `global` property of the same name, which overrides `max_byte_per_trace` for all tenants). For an example on how to enable the metrics generator using the `overrides` object, see the 'Activate metrics generator' section below. Refer to [Standard overrides](https://grafana.com/docs/tempo/latest/configuration/#standard-overrides) for more details.
    overrides:
      # -- Default config values for all tenants, can be overridden by per-tenant overrides. If a tenant's specific overrides are not found in the `per_tenant_overrides` block, the values in this `default` block will be used. Configs inside this block should follow the new overrides indentation format
      defaults: {}
      #  metrics_generator:
      #    processors:
      #      - service-graphs
      #      - span-metrics

      # -- Path to the per tenant override config file. The values of the `per_tenant_overrides` config below will be written to the default path `/conf/overrides.yaml`. Users can set tenant-specific overrides settings in a separate file and point per_tenant_override_config to it if not using the per_tenant_overrides block below.
      per_tenant_override_config: /conf/overrides.yaml
    # -- The `per tenant` aka `tenant-specific` runtime overrides. This allows overriding values set in the configuration on a per-tenant basis. Note that *all* values must be given for each per-tenant configuration block. Refer to [Runtime overrides](https://grafana.com/docs/tempo/latest/configuration/#runtime-overrides) and [Tenant-Specific overrides](https://grafana.com/docs/tempo/latest/configuration/#tenant-specific-overrides) documentation for more details.
    per_tenant_overrides: {}
      # 'tenant-id':
      #  metrics_generator:
      #    processors:
      #      - service-graphs
      #      - span-metrics

    # Tempo server configuration.
    # Refers to: https://grafana.com/docs/tempo/latest/configuration/#server
    server:
      # -- HTTP server listen port
      http_listen_port: 3200
    # Readiness and Liveness Probe Configuration Options
    livenessProbe:
      httpGet:
        path: /ready
        port: 3200
      initialDelaySeconds: 30
      periodSeconds: 10
      timeoutSeconds: 5
      failureThreshold: 3
      successThreshold: 1
    readinessProbe:
      httpGet:
        path: /ready
        port: 3200
      initialDelaySeconds: 20
      periodSeconds: 10
      timeoutSeconds: 5
      failureThreshold: 3
      successThreshold: 1
    storage:
      trace:
        # tempo storage backend.
        # Refers to: https://grafana.com/docs/tempo/latest/configuration/#storage
        ## Use s3 for example
        # backend: s3
        # store traces in s3
        # s3:
        #   bucket: <your s3 bucket>                        # store traces in this bucket
        #   endpoint: s3.dualstack.us-east-2.amazonaws.com  # api endpoint
        #   access_key: ...                                 # optional. access key when using static credentials.
        #   secret_key: ...                                 # optional. secret key when using static credentials.
        #   insecure: false                                 # optional. enable if endpoint is http
        backend: local
        local:
          path: /var/tempo/traces
        wal:
          path: /var/tempo/wal
    # this configuration will listen on all ports and protocols that tempo is capable of.
    # the receives all come from the OpenTelemetry collector.  more configuration information can
    # be found there: https://github.com/open-telemetry/opentelemetry-collector/tree/master/receiver
    receivers:
      jaeger:
        protocols:
          grpc:
            endpoint: 0.0.0.0:14250
          thrift_binary:
            endpoint: 0.0.0.0:6832
          thrift_compact:
            endpoint: 0.0.0.0:6831
          thrift_http:
            endpoint: 0.0.0.0:14268
      opencensus:
      otlp:
        protocols:
          grpc:
            endpoint: "0.0.0.0:4317"
          http:
            endpoint: "0.0.0.0:4318"
    securityContext: {}
      # allowPrivilegeEscalation: false
      #  capabilities:
      #    drop:
      #    - ALL
      # readOnlyRootFilesystem: true
    ## Additional container arguments
    extraArgs: {}
    # -- Environment variables to add
    extraEnv: []
    # -- Environment variables from secrets or configmaps to add to the ingester pods
    extraEnvFrom: []
    # -- Volume mounts to add
    extraVolumeMounts: []
    # - name: extra-volume
    #   mountPath: /mnt/volume
    #   readOnly: true
    #   existingClaim: volume-claim

  # -- Tempo configuration file contents
  # @default -- Dynamically generated tempo configmap
  config: |
      memberlist:
        cluster_label: "{{ .Release.Name }}.{{ .Release.Namespace }}"
      multitenancy_enabled: {{ .Values.tempo.multitenancyEnabled }}
      usage_report:
        reporting_enabled: {{ .Values.tempo.reportingEnabled }}
      compactor:
        compaction:
          block_retention: {{ .Values.tempo.retention }}
      distributor:
        receivers:
          {{- toYaml .Values.tempo.receivers | nindent 8 }}
      ingester:
        {{- toYaml .Values.tempo.ingester | nindent 6 }}
      server:
        {{- toYaml .Values.tempo.server | nindent 6 }}
      storage:
        {{- toYaml .Values.tempo.storage | nindent 6 }}
      querier:
        {{- toYaml .Values.tempo.querier | nindent 6 }}
      query_frontend:
        {{- toYaml .Values.tempo.queryFrontend | nindent 6 }}
      overrides:
        {{- toYaml .Values.tempo.overrides | nindent 6 }}
        {{- if .Values.tempo.metricsGenerator.enabled }}
      metrics_generator:
            storage:
              path: "/tmp/tempo"
              remote_write:
                - url: {{ .Values.tempo.metricsGenerator.remoteWriteUrl }}
            traces_storage:
              path: "/tmp/traces"
        {{- end }}

  tempoQuery:
    repository: grafana/tempo-query
    tag: null
    pullPolicy: IfNotPresent
    ## Optionally specify an array of imagePullSecrets.
    ## Secrets must be manually created in the namespace.
    ## Refers to: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
    ##
    # pullSecrets:
    #   - myRegistryKeySecretName

    # -- if False the tempo-query container is not deployed
    enabled: false

    service:
      port: 16686

    ingress:
      enabled: false
      # For Kubernetes >= 1.18 you should specify the ingress-controller via the field ingressClassName
      # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress
      # ingressClassName: nginx
      # Values can be templated
      annotations: {}
      # kubernetes.io/ingress.class: nginx
      # kubernetes.io/tls-acme: "true"
      labels: {}
      path: /

      # pathType is only for k8s >= 1.1=
      pathType: Prefix

      hosts:
        - query.tempo.example.com
      ## Extra paths to prepend to every host configuration. This is useful when working with annotation based services.
      extraPaths: []
      # - path: /*
      #   backend:
      #     serviceName: ssl-redirect
      #     servicePort: use-annotation
      ## Or for k8s > 1.19
      # - path: /*
      #   pathType: Prefix
      #   backend:
      #     service:
      #       name: ssl-redirect
      #       port:
      #         name: use-annotation


      tls: []
      #  - secretName: tempo-query-tls
      #    hosts:
      #      - query.tempo.example.com

    resources: {}
    #  requests:
    #    cpu: 1000m
    #    memory: 4Gi
    #  limits:
    #    cpu: 2000m
    #    memory: 6Gi

    ## Additional container arguments
    extraArgs: {}
    # -- Environment variables to add
    extraEnv: []
    # -- Volume mounts to add
    extraVolumeMounts: []
    # - name: extra-volume
    #   mountPath: /mnt/volume
    #   readOnly: true
    #   existingClaim: volume-claim
    securityContext: {}
      # allowPrivilegeEscalation: false
      #  capabilities:
      #    drop:
      #    - ALL
      # readOnlyRootFilesystem: false # fails if true, do not enable

  # -- securityContext for container
  securityContext:
    runAsUser: 10001
    runAsGroup: 10001
    fsGroup: 10001
    runAsNonRoot: true

  serviceAccount:
    # -- Specifies whether a ServiceAccount should be created
    create: true
    # -- The name of the ServiceAccount to use.
    # If not set and create is true, a name is generated using the fullname template
    name: null
    # -- Image pull secrets for the service account
    imagePullSecrets: []
    # -- Annotations for the service account
    annotations: {}
    # -- Labels for the service account
    labels: {}
    automountServiceAccountToken: true

  service:
    type: ClusterIP
    clusterIP: ""
    # -- (string) IP address, in case of 'type: LoadBalancer'
    loadBalancerIP:
    # -- If service type is LoadBalancer, the exposed protocol can either be "UDP", "TCP" or "UDP,TCP"
    protocol: "TCP"

    annotations: {}
    labels: {}
    targetPort: ""

  serviceMonitor:
    enabled: false
    interval: ""
    additionalLabels: {}
    annotations: {}
    # scrapeTimeout: 10s

  persistence:
    enabled: false
      # -- Enable StatefulSetAutoDeletePVC feature
    enableStatefulSetAutoDeletePVC: false
    # storageClassName: local-path
    accessModes:
      - ReadWriteOnce
    size: 10Gi

  # -- Pod Annotations
  podAnnotations: {}

  # -- Pod (extra) Labels
  podLabels: {}

  # Apply extra labels to common labels.
  extraLabels: {}

  # -- Volumes to add
  extraVolumes: []

  # -- Node labels for pod assignment. See: https://kubernetes.io/docs/user-guide/node-selection/
  nodeSelector: {}

  # -- Tolerations for pod assignment. See: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
  tolerations: []

  # -- Affinity for pod assignment. See: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
  affinity: {}

  # -- The name of the PriorityClass
  priorityClassName: null
  # -- hostAliases to add
  hostAliases: []
  #  - ip: 1.2.3.4
  #    hostnames:
  #      - domain.tld
  networkPolicy:
    ## @param networkPolicy.enabled Enable creation of NetworkPolicy resources. Only Ingress traffic is filtered for now.
    ##
    enabled: false
    ## @param networkPolicy.allowExternal Don't require client label for connections
    ## The Policy model to apply. When set to false, only pods with the correct
    ## client label will have network access to  tempo port defined.
    ## When true, tempo will accept connections from any source
    ## (with the correct destination port).
    ##
    ingress: true
    ## @param networkPolicy.ingress When true enables the creation
    ## an ingress network policy
    ##
    allowExternal: true
    ## @param networkPolicy.explicitNamespacesSelector A Kubernetes LabelSelector to explicitly select namespaces from which traffic could be allowed
    ## If explicitNamespacesSelector is missing or set to {}, only client Pods that are in the networkPolicy's namespace
    ## and that match other criteria, the ones that have the good label, can reach the tempo.
    ## But sometimes, we want the tempo to be accessible to clients from other namespaces, in this case, we can use this
    ## LabelSelector to select these namespaces, note that the networkPolicy's namespace should also be explicitly added.
    ##
    ## Example:
    ## explicitNamespacesSelector:
    ##   matchLabels:
    ##     role: frontend
    ##   matchExpressions:
    ##    - {key: role, operator: In, values: [frontend]}
    ##
    explicitNamespacesSelector: {}
    ##
    egress:
      ## @param networkPolicy.egress.enabled When enabled, an egress network policy will be
      ## created allowing tempo to connect to external data sources from kubernetes cluster.
      enabled: false
      ##
      ## @param networkPolicy.egress.blockDNSResolution When enabled, DNS resolution will be blocked
      ## for all pods in the tempo namespace.
      blockDNSResolution: false
      ##
      ## @param networkPolicy.egress.ports Add individual ports to be allowed by the egress
      ports: []
      ## Add ports to the egress by specifying - port: <port number>
      ## E.X.
      ## - port: 80
      ## - port: 443
      ##
      ## @param networkPolicy.egress.to Allow egress traffic to specific destinations
      to: []
      ## Add destinations to the egress by specifying - ipBlock: <CIDR>
      ## E.X.
      ## to:
      ##  - namespaceSelector:
      ##    matchExpressions:
    ##    - {key: role, operator: In, values: [tempo]}
prometheus:
  enabled: true
  rbac:
    create: true

  podSecurityPolicy:
    enabled: false

  serviceAccounts:
    server:
      create: true
      name: ""

  configmapReload:
    reloadUrl: ""
    prometheus:
      enabled: true
      name: configmap-reload
      image:
        repository: quay.io/prometheus-operator/prometheus-config-reloader
        tag: v0.78.1
        pullPolicy: IfNotPresent
      containerPort: 8080
      containerPortName: metrics
      livenessProbe:
        httpGet:
          path: /healthz
          port: metrics
          scheme: HTTP
        periodSeconds: 10
        initialDelaySeconds: 2
      readinessProbe:
        httpGet:
          path: /healthz
          port: metrics
          scheme: HTTP
        periodSeconds: 10
      startupProbe:
        enabled: false
        httpGet:
          path: /healthz
          port: metrics
          scheme: HTTP
        periodSeconds: 10

  server:
    name: server
    releaseNamespace: false
    image:
      repository: quay.io/prometheus/prometheus
      pullPolicy: IfNotPresent
    enableServiceLinks: true
    extraFlags:
      - web.enable-lifecycle
      - web.enable-remote-write-receiver
    configPath: /etc/config/prometheus.yml
    global:
      scrape_interval: 1m
      scrape_timeout: 10s
      evaluation_interval: 1m
    fullnameOverride: "prometheus"
    ingress:
      enabled: false
      path: /
      pathType: Prefix
    strategy:
      type: Recreate
    podAntiAffinityTopologyKey: kubernetes.io/hostname
    persistentVolume:
      enabled: true
      accessModes:
        - ReadWriteOnce
      mountPath: /data
      size: 8Gi
    replicaCount: 1
    readinessProbeInitialDelay: 30
    readinessProbePeriodSeconds: 5
    readinessProbeTimeout: 4
    readinessProbeFailureThreshold: 3
    readinessProbeSuccessThreshold: 1
    livenessProbeInitialDelay: 30
    livenessProbePeriodSeconds: 15
    livenessProbeTimeout: 10
    livenessProbeFailureThreshold: 3
    livenessProbeSuccessThreshold: 1
    startupProbe:
      enabled: false
      periodSeconds: 5
      failureThreshold: 30
      timeoutSeconds: 10
    hostNetwork: false
    dnsPolicy: ClusterFirst
    securityContext:
      runAsUser: 65534
      runAsNonRoot: true
      runAsGroup: 65534
      fsGroup: 65534
    service:
      enabled: true
      servicePort: 80
      sessionAffinity: None
      type: ClusterIP
    terminationGracePeriodSeconds: 300
    retention: "15d"

  serverFiles:
    prometheus.yml:
      rule_files:
        - /etc/config/recording_rules.yml
        - /etc/config/alerting_rules.yml
        - /etc/config/rules
        - /etc/config/alerts
      scrape_configs:
        - job_name: prometheus
          static_configs:
            - targets:
                - localhost:9090

  alertmanager:
    enabled: false

  kube-state-metrics:
    enabled: false

  prometheus-node-exporter:
    enabled: false

  prometheus-pushgateway:
    enabled: false

grafana:
  enabled: true
  fullnameOverride: grafana
  global:
    imageRegistry: null
    imagePullSecrets: []
  rbac:
    create: true
    pspEnabled: false
    pspUseAppArmor: false
    namespaced: true

  serviceAccount:
    create: true
    name:
    nameTest:
    labels: {}
    automountServiceAccountToken: false

  replicas: 1

  headlessService: false

  automountServiceAccountToken: true

  autoscaling:
    enabled: false

  deploymentStrategy:
    type: RollingUpdate

  readinessProbe:
    httpGet:
      path: /api/health
      port: 3000

  livenessProbe:
    httpGet:
      path: /api/health
      port: 3000
    initialDelaySeconds: 60
    timeoutSeconds: 30
    failureThreshold: 10

  image:
    registry: docker.io
    repository: grafana/grafana
    pullPolicy: IfNotPresent
  
  testFramework:
    enabled: false
    
  securityContext:
    runAsNonRoot: true
    runAsUser: 472
    runAsGroup: 472
    fsGroup: 472

  containerSecurityContext:
    allowPrivilegeEscalation: false
    capabilities:
      drop:
      - ALL
    seccompProfile:
      type: RuntimeDefault
  createConfigmap: true
  extraConfigmapMounts: []
    # - name: certs-configmap
    #   mountPath: /etc/grafana/ssl/
    #   subPath: certificates.crt # (optional)
    #   configMap: certs-configmap
    #   readOnly: true
    #   optional: false

  extraLabels: {}


  downloadDashboardsImage:
    # -- The Docker registry
    registry: docker.io
    repository: curlimages/curl
    tag: 7.85.0
    sha: ""
    pullPolicy: IfNotPresent

  downloadDashboards:
    env: {}
    envFromSecret: ""
    resources: {}
    securityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
        - ALL
      seccompProfile:
        type: RuntimeDefault
    envValueFrom: {}

  podPortName: grafana
  gossipPortName: gossip

  service:
    enabled: true
    type: ClusterIP
    port: 80
    targetPort: 3000
    annotations: {}
    labels: {}
    portName: service
    appProtocol: ""


  ingress:
    enabled: true
    ingressClassName: traefik
    annotations: {}
    labels: {}
    path: /
    pathType: Prefix
    hosts:
      - grafana.k8s.test
    backend:
      service:
        name: grafana
        port:
          number: 80


  serviceMonitor:
    enabled: false
    path: /metrics
    labels: {}
    interval: 30s
    scheme: http
    tlsConfig: {}
    scrapeTimeout: 30s
    relabelings: []
    metricRelabelings: []
    targetLabels: []

  extraExposePorts: []
  resources: {}

  persistence:
    type: pvc
    enabled: false
    accessModes:
      - ReadWriteOnce
    size: 1Gi
    finalizers:
      - kubernetes.io/pvc-protection
    extraPvcLabels: {}
    disableWarning: false

  adminUser: admin

  plugins: []
    # - digrich-bubblechart-panel
    # - grafana-clock-panel
  datasources:
    datasources.yaml:
      apiVersion: 1
      datasources:
      - editable: true
        isDefault: true
        name: Prometheus
        type: prometheus
        uid: prometheus
        url: http://prometheus
      - editable: true
        isDefault: false
        name: loki
        type: loki
        uid: loki
        url: http://loki-gateway
        jsonData:
          httpHeaderName1: "X-Scope-OrgID"
        secureJsonData:
          httpHeaderValue1: "foo"
      - editable: true
        isDefault: false
        name: loki-bazz
        type: loki
        uid: loki-bazz
        url: http://loki-gateway
        jsonData:
          httpHeaderName1: "X-Scope-OrgID"
        secureJsonData:
          httpHeaderValue1: "bazz"
      - name: Tempo
        type: tempo
        uid: tempo
        url: http://tempo:3200
        jsonData:
          tracesToLogsV2:
            customQuery: true
            datasourceUid: 'loki'
            query: '{$${__tags}} | trace_id = "$${__trace.traceId}"'
            tags:
              - key: 'service.name'
                value: 'service_name'
  #    deleteDatasources: []
  #    - name: Prometheus

  alerting: {}
  notifiers: {}
  dashboardProviders: {}
  dashboards: {}
    # default:
    #   some-dashboard:
    #     json: |
    #       $RAW_JSON
    #   custom-dashboard:
    #     file: dashboards/custom-dashboard.json
    #   prometheus-stats:
    #     gnetId: 2
    #     revision: 2
    #     datasource: Prometheus
    #   local-dashboard:
    #     url: https://example.com/repository/test.json
    #     token: ''
    #   local-dashboard-base64:
    #     url: https://example.com/repository/test-b64.json
    #     token: ''
    #     b64content: true
    #   local-dashboard-gitlab:
    #     url: https://example.com/repository/test-gitlab.json
    #     gitlabToken: ''
    #   local-dashboard-bitbucket:
    #     url: https://example.com/repository/test-bitbucket.json
    #     bearerToken: ''
    #   local-dashboard-azure:
    #     url: https://example.com/repository/test-azure.json
    #     basic: ''
    #     acceptHeader: '*/*'

  dashboardsConfigMaps: {}
  #  default: ""
  grafana.ini:
    paths:
      data: /var/lib/grafana/
      logs: /var/log/grafana
      plugins: /var/lib/grafana/plugins
      provisioning: /etc/grafana/provisioning
    analytics:
      check_for_updates: true
    log:
      mode: console
    auth:
      disable_login: true
    auth.anonymous:
      enabled: true
      org_role: "Admin"
    grafana_net:
      url: https://grafana.net
    server:
      domain: "{{ if (and .Values.ingress.enabled .Values.ingress.hosts) }}{{ tpl (.Values.ingress.hosts | first) . }}{{ else }}''{{ end }}"

    profile:
      enabled: false

    feature_toggles:
      alertingListViewV2: true
      grafanaAdvisor: true
      ProfileFeatureTogglePage: false
  ldap:
    enabled: false
    existingSecret: ""
    config: ""

  revisionHistoryLimit: 10
minio:
  enabled: false  # Disabled - using local filesystem storage instead
  fullnameOverride: minio
  auth:
    rootUser: "minio"
    rootPassword: "minio123"
  defaultBuckets: "tempo-traces,loki-chunks,loki-ruler,loki-admin"
  persistence:
    enabled: false
  resources:
    requests:
      cpu: "100m"
      memory: "128Mi"
    limits:
      cpu: "250m"
      memory: "256Mi"

opentelemetry-collector:
  enabled: true
  fullnameOverride: "otel-collector"
  
  mode: "deployment"
  
  image:
    repository: "otel/opentelemetry-collector-contrib"
    tag: "latest"
    pullPolicy: "IfNotPresent"
  
  command:
    name: "otelcol-contrib"
  
  config:
    receivers:
      otlp:
        protocols:
          grpc:
            endpoint: 0.0.0.0:4317
          http:
            endpoint: 0.0.0.0:4318
    
    processors:
      batch: {}
      memory_limiter:
        limit_mib: 128
    connectors:
      routing:
        default_pipelines: [logs/default]
        table:
          - context: log
            statement: |
              route() where
                attributes["dev.audit.*"] != nil or
            pipelines: [logs/audit]

    exporters:
      debug/default:
        verbosity: basic
      
      debug/audit: 
        verbosity: detailed
      # Traces to Tempo
      otlp/tempo:
        endpoint: http://tempo.observability-lab.svc.cluster.local:4317
        tls:
          insecure: true
      
      # Logs to Loki
      otlphttp/default:
        logs_endpoint: http://loki-gateway.observability-lab.svc.cluster.local/otlp/v1/logs
        headers:
          "X-Scope-OrgID": foo
      # Logs to Loki
      otlphttp/bazz:
        logs_endpoint: http://loki-gateway.observability-lab.svc.cluster.local/otlp/v1/logs
        headers:
          "X-Scope-OrgID": bazz
      # Metrics to Prometheus remote write
      prometheusremotewrite:
        endpoint: http://prometheus.observability-lab.svc.cluster.local:80/api/v1/write
        tls:
          insecure: true
    
    service:
      pipelines:
        traces:
          receivers: [otlp]
          processors: [memory_limiter, batch]
          exporters: [otlp/tempo]
        
        logs:
          receivers: [otlp]
          processors: [memory_limiter, batch]
          exporters: [routing]
        
        metrics:
          receivers: [otlp]
          processors: [memory_limiter, batch]
          exporters: [prometheusremotewrite]
        logs/default:
          receivers: [routing]
          processors: [memory_limiter, batch]
          exporters: [otlphttp/default]
        logs/audit:
          receivers: [routing]
          processors: [memory_limiter, batch]
          exporters: [otlphttp/bazz, debug/audit]


  resources:
    requests:
      cpu: "100m"
      memory: "128Mi"
    limits:
      cpu: "250m"
      memory: "256Mi"

  ingress:
    enabled: true
    ingressClassName: "traefik"
    annotations: {}
    hosts:
      - host: otel-collector.k8s.test
        paths:
          - path: /
            pathType: Prefix
            port: 4318
